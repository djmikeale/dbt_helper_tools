{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Model Renaming and File Update in dbt Projects\n",
    "\n",
    "*Do you have a lot of legacy models that don't follow best naming practices?*\n",
    "\n",
    "This script streamlines the process of updating model names and related files within a dbt (Data Build Tool) project. \n",
    "\n",
    "\n",
    "\n",
    "- **Prefix Enforcement:** Identifies models in a specific layer that do not adhere to the intended prefix convention.\n",
    "- **Model Renaming:** Automatically renames models to align with the latest best practices.\n",
    "- **File Synchronization:** Updates the names and contents of corresponding `.yml` and `.md` files (if they exist).\n",
    "- **Reference Management:** Adjusts references in downstream models to ensure they point to the newly renamed models.\n",
    "\n",
    "**Important Considerations:**\n",
    "\n",
    "- The script only processes `.yml` files, not `.yaml` files.\n",
    "- It assumes `.yml` files share the same name and directory as their associated SQL files.\n",
    "- Downstream models may still use old aliases, which aren't updated automatically. These instances will be flagged in the output for manual correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "repo_path = '/Users/mikaelthorup/repos/lunar/lunar-way-hubble-transformations'\n",
    "manifest_path = f'{repo_path}/target/prod/manifest.json'\n",
    "all_model_path = f'{repo_path}/models'\n",
    "project_identifier = 'model.lw_go_events'\n",
    "schema_prefix = 'e_'\n",
    "schema_name = 'public_events'\n",
    "test_sample_size = None # Set to None to use all data / Set to a number to use a sample of the data\n",
    "old_model_views_to_keep = ['transaction_posted','user_suspended','userid_to_marketuserid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(manifest_path, 'r') as file:\n",
    "    manifest_data = json.load(file)\n",
    "\n",
    "# Prepare data for the DataFrame\n",
    "table_data = []\n",
    "for key, value in manifest_data.get('nodes', {}).items():\n",
    "    if value.get('unique_id').startswith(project_identifier):\n",
    "        table_data.append({\n",
    "            'unique_id': value.get('unique_id'),\n",
    "            'old_sql_path': value.get('original_file_path'),\n",
    "            'old_model_name': value.get('name'),\n",
    "            'schema_name': value.get('schema')\n",
    "        })\n",
    "\n",
    "# Create the DataFrame\n",
    "df_all = pd.DataFrame(table_data, columns=['unique_id','old_model_name','old_sql_path', 'schema_name'])\n",
    "df = df_all.copy()\n",
    "if test_sample_size:\n",
    "    df = df.head(test_sample_size)\n",
    "\n",
    "df = df[~df['old_model_name'].str.startswith(schema_prefix) & (df['schema_name'] == schema_name)]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['new_model_name'] = df['old_model_name'].apply(lambda x: schema_prefix + x)\n",
    "df['new_sql_path'] = df['old_sql_path'].apply(lambda x: x.rsplit('/', 1)[0] + '/' + schema_prefix + x.rsplit('/', 1)[1])\n",
    "df['old_yml_path'] = df['old_sql_path'].apply(lambda x: x[:-3] + 'yml')\n",
    "df['old_yml_path'] = df['old_yml_path'].apply(lambda x: x if os.path.exists(repo_path + '/' + x) else None)\n",
    "df['new_yml_path'] = df['new_sql_path'].apply(lambda x: x[:-3] + 'yml')\n",
    "df['new_yml_path'] = df.apply(lambda row: None if pd.isnull(row['old_yml_path']) else row['new_yml_path'], axis=1)\n",
    "df['old_md_path'] = df['old_sql_path'].apply(lambda x: x[:-3] + 'md')\n",
    "df['old_md_path'] = df['old_md_path'].apply(lambda x: x if os.path.exists(repo_path + '/' + x) else None)\n",
    "df['new_md_path'] = df['new_sql_path'].apply(lambda x: x[:-3] + 'md')\n",
    "df['new_md_path'] = df.apply(lambda row: None if pd.isnull(row['old_md_path']) else row['new_md_path'], axis=1)\n",
    "df['keep_old_model'] = df['old_model_name'].apply(lambda x: x in old_model_views_to_keep)\n",
    "# Display the DataFrame\n",
    "df = df[[\n",
    "    'old_model_name', 'new_model_name',\n",
    "    'old_sql_path', 'new_sql_path',\n",
    "    'old_yml_path', 'new_yml_path',\n",
    "    'old_md_path', 'new_md_path',\n",
    "    'unique_id', 'schema_name',\n",
    "    'keep_old_model']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the dictionary into a list of tuples\n",
    "data_tuples = [(old_model_name, child_model_name) for old_model_name, child_model_name in manifest_data['child_map'].items()]\n",
    "\n",
    "# Create a DataFrame from the list of tuples\n",
    "df_child_map = pd.DataFrame(data_tuples, columns=['old_model_name', 'child_model_name'])\n",
    "df_child_map = df_child_map[df_child_map['old_model_name'].isin(df['unique_id'])]\n",
    "\n",
    "df_child_map = df_child_map.explode('child_model_name')\n",
    "df_child_map = df_child_map.dropna()\n",
    "\n",
    "# keep only nodes that are models\n",
    "df_child_map = df_child_map[df_child_map['child_model_name'].str.startswith(project_identifier)]\n",
    "df_child_map = df_child_map.reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_child_map['old_model_name'] = df_child_map['old_model_name'].apply(lambda x: x[len(project_identifier)+1:])\n",
    "df_child_map['new_model_name'] = df_child_map['old_model_name'].apply(lambda x: schema_prefix + x)\n",
    "\n",
    "df_child_map = df_child_map[['child_model_name','old_model_name', 'new_model_name']]\n",
    "\n",
    "df_child_map = df_child_map.merge(df_all[['unique_id', 'old_sql_path']], left_on='child_model_name', right_on='unique_id', how='left')\n",
    "df_child_map = df_child_map.drop(columns='unique_id')\n",
    "df_child_map = df_child_map.rename(columns={'old_sql_path': 'child_sql_path'})\n",
    "df_child_map['is_in_df'] = df_child_map['child_model_name'].isin(df['unique_id'])\n",
    "df_child_map.loc[df_child_map['is_in_df'], 'child_sql_path'] = df_child_map.loc[df_child_map['is_in_df'], 'child_sql_path'].apply(lambda x: x.rsplit('/', 1)[0] + '/' + schema_prefix + x.rsplit('/', 1)[1])\n",
    "df_child_map = df_child_map[['child_model_name', 'child_sql_path', 'old_model_name', 'new_model_name','is_in_df']]\n",
    "df_child_map.sort_values(by='child_model_name', inplace=True)\n",
    "df_child_map.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    old_path = os.path.join(repo_path, row['old_sql_path'])\n",
    "    new_path = os.path.join(repo_path, row['new_sql_path'])\n",
    "    os.rename(old_path, new_path)\n",
    "\n",
    "    if row['old_yml_path']:\n",
    "        old_path = os.path.join(repo_path, row['old_yml_path'])\n",
    "        new_path = os.path.join(repo_path, row['new_yml_path'])\n",
    "        os.rename(old_path, new_path)\n",
    "\n",
    "    if row['old_md_path']:\n",
    "        old_path = os.path.join(repo_path, row['old_md_path'])\n",
    "        new_path = os.path.join(repo_path, row['new_md_path'])\n",
    "        os.rename(old_path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if row['old_yml_path']:\n",
    "        new_path = os.path.join(repo_path, row['new_yml_path'])\n",
    "\n",
    "        with open(new_path, 'r+') as file:\n",
    "            content = file.read()\n",
    "            content = content.replace(\"  - name: \" + row['old_model_name'], \"  - name: \" + row['new_model_name'])\n",
    "            file.seek(0)\n",
    "            file.write(content)\n",
    "            file.truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace downstream refs in child models with new model names\n",
    "old_ref_found = 0\n",
    "for index, row in df_child_map.iterrows():\n",
    "    path = os.path.join(repo_path, row['child_sql_path'])\n",
    "    with open(path, 'r+') as file:\n",
    "            content = file.read()\n",
    "            from_str = r\"\\{\\{.*['\\\"]\" + re.escape(row['old_model_name']) + r\"['\\\"].*\\}\\}\"\n",
    "            to_str = \"{{ ref('\" + row['new_model_name'] + \"') }}\"\n",
    "            content = re.sub(from_str, to_str, content)\n",
    "            # note the following regex might produce false positives\n",
    "            old_model_re = r\"(?<!{}){}\".format(re.escape(schema_prefix), re.escape(row['old_model_name']))\n",
    "            if re.search(old_model_re, content):\n",
    "                old_ref_found += 1\n",
    "                if old_ref_found == 1:\n",
    "                    print(\"one or more files found with reference to old model name, please correct manually\")\n",
    "                    print(\"path, old_model_name\")\n",
    "                print(f\"{path},{row['old_model_name']}\")\n",
    "            file.seek(0)\n",
    "            file.write(content)\n",
    "            file.truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df[df['keep_old_model']].iterrows():\n",
    "    sql_text = \"\"\"{{ config(\n",
    "    materialized='view',\n",
    "    schema='events'\n",
    ")}}\n",
    "\n",
    "-- this model is a temp model to keep the old model name.\n",
    "\n",
    "\n",
    "-- THIS MODEL IS DEPRECATED (ノಠ益ಠ)ノ彡┻━┻\n",
    "-- USE UPSTREAM MODEL FOR NEW QUERIES\n",
    "\n",
    "\n",
    "\n",
    "select * from {{ ref('\"\"\" + row['new_model_name'] + \"\"\"') }}\"\"\"\n",
    "    yml_text = \"\"\"version: 2\n",
    "models:\n",
    "  - name: \"\"\" + row['old_model_name'] + \"\"\"\n",
    "    description: |\n",
    "      this model is a temp model to keep the old model name.\n",
    "      It should be deleted after all consumers have shifted away.\n",
    "\n",
    "    meta:\n",
    "      domain: not_applicable\n",
    "      owner: data_warehouse\n",
    "      criticality: low\n",
    "      contains_pii: false\n",
    "\n",
    "    deprecation_date: 2024-10-01\n",
    "    \"\"\"\n",
    "    with open(os.path.join(repo_path, row['old_sql_path']), 'w') as file:\n",
    "        file.write(sql_text)\n",
    "\n",
    "    with open(os.path.join(repo_path, row['old_sql_path'][:-3] + 'yml'), 'w') as file:\n",
    "        file.write(yml_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
